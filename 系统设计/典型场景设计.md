# 典型场景设计

## 分布式 ID 服务

分布式集群中生成不冲突的唯一 id。

常用的设计思路：

- Snowflake 方案：本地 SDK 生成，使用 snowflake 算法，依赖中央节点分配唯一的机器 id。
- 单独服务方案：提供统一的分布式 id 生成服务，通常使用数据库 CAS 预申请号段，然后内存中做分发。如美团的 Leaf、滴滴的 Tinyid。
- 本地 SDK 号段方案：本地 SDK 依赖中央存储，申请号段，然后内存中做分发。如蚂蚁的 ZDAL Sequence。

各个方案都是生成趋势递增的 id，保证吞吐量。其中对于存在中央服务节点的，要做集群化保证中央服务的可用性。

Snowflake 是 Twitter 提供的分布式 id 生成算法，它使用一个 long 来表示分布式 id，其中默认的各比特位（从高到低）存储的信息如下：

- 1 bit：不使用
- 41 bit：时间戳，从某个时间点开始的毫秒数，大于能表示69年。
- 10 bit：机器 id，能支持的最大集群规模为 `2^10=1024`。
- 12 bit：在同一个时间戳下从0递增的编号，则一毫秒可以生成 `2^12=4096` 个，通常够用了（CPU 频率限制）。

可以根据实际需要，调整各比特段使用的长度。

Snowflake 方案总结：

- 每台机器获取唯一的机器 id，可以通过配置或中央存储（如 ZK）来获取。
- 每次 id 生成时，不依赖其他服务，无中心节点，可用性和吞吐量都很高。
- 依赖本地时钟，可能会产生时钟回拨问题。通常的解决方案有：
    - 当发生时钟回拨（当前时间小于内存里上次生成 id 使用的时间）时，继续使用之前 id 的时间，用完每个时间戳内的号段（4096个）为止，让当前时间慢慢追上内存内的时间戳。

美团 Leaf 方案总结：

- 使用数据库的一张 sequence 表，每次 CAS 申请一个新的号段。每次请求使用内存中号段的数据提供服务，不用每次访问数据库。
- 不同的业务可以提供不同的 sequence 步长，提高步长可以在数据库不可用时保证服务可用性的时间，但是可能造成 id 的空洞变大。
- 避免号段用完时加载数据库影响吞吐量，在内存中保存两个号段用来分发，当用完一个号段后，立刻使用下一个提供服务，并异步的加载。

## 短链接服务

短链接服务，为一个长链接生成一个短链接，用户访问短链接时重定向到对应的真实地址。

核心思路：发号器+进制转换+缓存。

为什么不使用 hash 算法？

- hash 可能会出现冲突，因此还是需要持久化存储，如果必须使用存储，不如直接不使用 hash。

短链接是一个字符+数字组成的字符串，本质上也可以视为一个多进制数。如使用数字(10)+大小写字母(26*2)，则相对于一个62进制的数，6位长度的短链接能表示 `62^6`，大约 568E，应该够用了。由于本质是数字，可以通过进制转换后使用 long 来做存储和查询，效率会很高。

如何生成一个不重复的数字，那么问题就转换为分布式 id 生成问题了，可以参考分布式 id 的方案。

数据的存储：

- 数据库中使用一个表来存储，短链使用类型为 long 的 uk 列进行存储；对应的长链接使用 varchar 存储，不需要索引，因为通过长链接反查短链接不是必须的（同样的长链接生成重复的短链通常业务是可接受的）。可以通过缓存来提高吞吐量。
- 为了避免每次相同的长链都生成一条新的短链，可以通过 Redis 缓存长链到短链的映射，这样对于热门或者短期内重复生成的长链，可以生成同一个短链接地址。

使用不同重定向的区别？

- 301：永久重定向，浏览器会缓存，下次直接请求长链，减轻服务端压力。
- 302：临时重定向，浏览器不会缓存，每次都会请求短链，这样业务可以统计短链接的真实点击数。通常业界使用 302 的会多一些。

一些其他的优化：

- 如何避免发号规律被发现？可以在结果中插入一些随机的比特，请求的时候去掉这些随机比特再查询。
- 发现无效请求？增加校验位，先校验合法性。

## 热点 Key 问题

某个 Key 数据流量超过单个 Redis 实例的处理瓶颈（如 10W QPS），或者物理网卡的上限，导致缓存服务无法支撑。

核心思路：热点探测+本地缓存。

热点 Key 的发现：

- 提前预测热点 Key：对于秒杀类的商品，在活动前就知道将会成为热点 key。
- Client SDK 上报统计：统计的方式有很多，比如本地计数、发送消息、异步分析日志等。

热点 Key 的处理：

- 多写来分担读压力：如将一个 key 写多个副本到不同的 redis 节点，client 随机访问一个节点。多写维护成本比较高，也容易出现不一致问题。
- 将热点 key 提升到本地缓存：使用本地缓存来缓存热点 key 数据，增加了数据不一致的时间。

有赞的 TMC（Transparent Multilevel Cache）多级缓存处理方案：

- key 访问上报通过 client 记录日志，然后异步上报，不会阻塞业务。
- 热点 Key 上报后由统一服务进行统计和热点 key 探测，将热点 key 推送到本地 client。
- 本地缓存对内存大小进行管控，使用 64M 的 LRU Cache，杜绝 JVM 堆溢出的可能。
- 失效通过 etcd 集群广播来失效本地缓存，来保证集群的最终一致性。

## 抢红包

将 M 金额的红包发给 N 个人来抢，并且满足：

- 所有人抢到的金额刚好等于 M。
- 每人至少抢到一分钱。
- 每个人抢到金额的几率相当。

核心思路：概率分配算法，缓存挡流量，数据库CAS保证一致性，sharding 水平扩展。

二倍均值法：

- 剩余红包金额 M，剩余人数为 N，那么则每次抢到的金额在区间 `(0, M/N*2)` 间随机。每次抢，随机区间都是剩余每人均值的两倍，保证每次随机金额的平均值是相同的。

使用该方案缺点是，除了最后一次，任意一次抢的金额都小于人均金额的两倍，不是任意的随机。

抢红包的过程：

- 新建红包：在缓存和数据库中都新建一条记录。
- 抢红包：访问缓存，剩余红包个数大于0，则展示拆开红包按钮。
- 拆红包：访问缓存，剩余红包个数大于0，则根据剩余金额和剩余个数计算抢到的金额。CAS 更新数据库数据（更新剩余个数、金额，插入领取记录），CAS 成功，更新缓存；CAS 失败，循环拆红包的流程，直到成功或者剩余金额为0。

核心数据结构：

- 数据库：
    - 红包记录：红包总金额、红包总个数、红包剩余金额、红包剩余个数。
    - 领取记录表：实际领取的红包金额和次序。
- 缓存：
    - 红包记录：红包剩余金额、红包剩余个数。

核心优化：

- 数据库和缓存都可以进行 sharding，水平扩展。
- 红包数据是实时计算的，实时效率已经很高，没必要提前分配，不需要预先存储，浪费空间。
- 红包缓存数据只有一条记录，不需要队列，而且红包还有过期时间，不需要太多的空间。

## 秒杀系统设计

固定时间开始秒杀一个商品，库存数量有限。

核心思路：

- 由于最终成交的人数远小于秒杀参与的人数，尽量在上游拦截请求。前端拦截，网关拦截，缓存拦截，异步消费，存储更新保证一致性。

优化细节：

- 前端：
    - 页面元素静态化，减少动态内容，使用 CDN 减少压力。
    - 减少重复查询：
        - 前端控制，点击后置灰一定时间，减少80%流量。
        - 后端控制，防止恶意用户，如 uid 频率限制，x 秒内只返回一次数据。
- 网关：
    - 请求限流，如根据 IP 限流。
- 后端：
    - 读请求，使用缓存挡掉大量的无效请求，如排队队列满后，直接返回失败。
    - 写请求，由于库存优先，透过去过多的请求没有意义，可以使用缓存做计数器（可以比库存多一定阀值），使用缓存或者消息中间件作为消息队列，消费者异步处理数据，CAS 修改数据库。前端轮询入队成功的抢购结果。
